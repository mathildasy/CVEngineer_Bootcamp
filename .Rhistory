lines(t, predict(fit3,data1), col = "red")
x2 = x^2
x3 = x^3
x4 = x^4
x5 = x^5
x6 = x^6
x7 = x^7
x8 = x^8
x9 = x^9
fit4 = lm(y~x + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9)
data2 = data.frame(x = t, x2 = t^2, x3 = t^3, x4 = t^4, x5 = t^5, x6 = t^6,
x7 = t^7, x8 = t^8, x9 = t^9)
plot(t,ty,type="l", xlab="time", ylab="Sine wave", ylim = c(-3,3), main="M=9")
points(x = x,
y = y,
pch = 16,
col = 2)
lines(t, predict(fit4,data2), col = "red")
set.seed(2021)
par(mfrow = c(2,2))
x = seq(0, 0.9, by=0.1)
noise = rnorm(length(x), mean = 0, sd = 1)
y <- sin(2*pi*x)+noise
y_noiseless <- sin(2*pi*x)
t=seq(0,1,0.01)
ty=sin(2*pi*t)
plot(t,ty,type="l", xlab="time", ylab="Sine wave", ylim = c(-3,3), main="M=0")
points(x = x,
y = y,
pch = 16,
col = 2)
#plot(x, y, type='l', col='darkblue')
fit1 = mean(y)
abline(a = fit1, b = 0, col = 2)
plot(t,ty,type="l", xlab="time", ylab="Sine wave", ylim = c(-3,3), main="M=1")
points(x = x,
y = y,
pch = 16,
col = 2)
fit2 = lm(y~x)
abline(a = fit2$coefficients[1], b = fit2$coefficients[2], col = 2)
x2 = x^2
fit3 = lm(y~x + x2)
data1 = data.frame(x = t, x2 = t^2)
plot(t,ty,type="l", xlab="time", ylab="Sine wave", ylim = c(-3,3), main="M=2")
points(x = x,
y = y,
pch = 16,
col = 2)
lines(t, predict(fit3,data1), col = "red")
x2 = x^2
x3 = x^3
x4 = x^4
x5 = x^5
x6 = x^6
x7 = x^7
x8 = x^8
x9 = x^9
fit4 = lm(y~x + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9)
data2 = data.frame(x = t, x2 = t^2, x3 = t^3, x4 = t^4, x5 = t^5, x6 = t^6,
x7 = t^7, x8 = t^8, x9 = t^9)
plot(t,ty,type="l", xlab="time", ylab="Sine wave", ylim = c(-3,3), main="M=9")
points(x = x,
y = y,
pch = 16,
col = 2)
lines(t, predict(fit4,data2), col = "red")
plot(t,ty,type="l", xlab="time", ylab="Sine wave", ylim = c(-3,3), main="M=0")
points(x = x,
y = y_noiseless,
pch = 16,
col = 2)
#plot(x, y, type='l', col='darkblue')
fit1 = mean(y_noiseless)
abline(a = fit1, b = 0, col = 2)
plot(t,ty,type="l", xlab="time", ylab="Sine wave", ylim = c(-3,3), main="M=1")
points(x = x,
y = y_noiseless,
pch = 16,
col = 2)
fit2 = lm(y_noiseless~x)
abline(a = fit2$coefficients[1], b = fit2$coefficients[2], col = 2)
x2 = x^2
fit3 = lm(y_noiseless~x + x2)
data1 = data.frame(x = t, x2 = t^2)
plot(t,ty,type="l", xlab="time", ylab="Sine wave", ylim = c(-3,3), main="M=2")
points(x = x,
y = y_noiseless,
pch = 16,
col = 2)
lines(t, predict(fit3,data1), col = "red")
x2 = x^2
x3 = x^3
x4 = x^4
x5 = x^5
x6 = x^6
x7 = x^7
x8 = x^8
x9 = x^9
fit4 = lm(y_noiseless~x + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9)
data2 = data.frame(x = t, x2 = t^2, x3 = t^3, x4 = t^4, x5 = t^5, x6 = t^6,
x7 = t^7, x8 = t^8, x9 = t^9)
plot(t,ty,type="l", xlab="time", ylab="Sine wave", ylim = c(-3,3), main="M=9")
points(x = x,
y = y_noiseless,
pch = 16,
col = 2)
lines(t, predict(fit4,data2), col = "red")
rm(list=ls())
setwd("~/GitHub/CVEngineer_Bootcamp")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
senti <- read.csv("~/GitHub/CVEngineer_Bootcamp/sentiment_sentence.csv")
char <- read.csv("~/GitHub/CVEngineer_Bootcamp/character_sentence.csv")
ends <- read.csv("~/GitHub/CVEngineer_Bootcamp/chapter_ends.csv")
View(ends)
senti[,1]
senti[,2]
senti[,1]
senti_invest = senti[2,-1]
library(matlib)
contribution = matrix(0L, nrow = dim(char[,-1])[1], ncol = dim(char[,-1])[2])
for (i in 1:dim(char)[1]){
char_app = which(char[i,-1] == 1)
contribution[i, char_app] = sapply(char_app, kernel)
}
x = seq(-10, 10, by = 1)
y = 10*dnorm(x, mean = 0, sd = 1)
kernel <- function(id, one_d_data = senti_invest, weight = y, window = 21){
smooth = 0
if (id >= (window-1)/2 & id <= (dim(one_d_data)[2] - (window-1)/2)){
smooth = sum(weight * one_d_data[1,(id-(window-1)/2):(id+(window-1)/2)])
} else if (id < (window-1)/2) {
smooth = sum(weight[((window+1)/2-id):window] * one_d_data[1,1:(id+(window-1)/2)])
} else {
length  = (window+1)/2 + dim(one_d_data)[2]-id
smooth = sum(weight[1:length] * one_d_data[1,id-((window-1)/2):id-((window-1)/2)+length-1])
}
return(smooth)
}
library(matlib)
contribution = matrix(0L, nrow = dim(char[,-1])[1], ncol = dim(char[,-1])[2])
for (i in 1:dim(char)[1]){
char_app = which(char[i,-1] == 1)
contribution[i, char_app] = sapply(char_app, kernel)
}
# contribution 14*5584
# ridge regression
lambdas = 10^seq(2,-3, by= -0.1)
cost = 0
for (lambda in lambdas){
y = t(data.matrix(senti_invest))
y_hat = t(contribution) %*% inv(contribution %*% t(contribution) + lambda * diag(dim(contribution)[1])) %*% contribution %*% y
cost = c(cost ,norm(y-y_hat, type= "2"))
}
cost = cost[-1]
lambda_choose = lambdas[which.min(cost)]
beta = inv(contribution %*% t(contribution) + lambda_choose * diag(dim(contribution)[1])) %*% contribution %*% y
beta = beta/norm(beta, type='2')
plot(beta)
char[,1]
View(ends)
rang_invest = ends[1:3,2]
rang_invest = ends[(1):(3)+1,2]
rang_invest = ends[(1):(3)+1,2]
rang_invest = ends[(1):(3+1),2]
rang_invest = ends[(2):(4+1),2]
rang_invest = ends[(2):(10+1),2]
rang_invest = ends[c((2),(10+1)),2]
range_invest[1]
rang\_invest[1]
rang_invest[1]
range_invest = ends[c((2),(10+1)),2]
x = seq(-10, 10, by = 1)
y = 10*dnorm(x, mean = 0, sd = 1)
kernel <- function(id, one_d_data = senti_invest, weight = y, window = 21){
smooth = 0
if (id >= (window-1)/2 & id <= (dim(one_d_data)[2] - (window-1)/2)){
smooth = sum(weight * one_d_data[1,(id-(window-1)/2):(id+(window-1)/2)])
} else if (id < (window-1)/2) {
smooth = sum(weight[((window+1)/2-id):window] * one_d_data[1,1:(id+(window-1)/2)])
} else {
length  = (window+1)/2 + dim(one_d_data)[2]-id
smooth = sum(weight[1:length] * one_d_data[1,id-((window-1)/2):id-((window-1)/2)+length-1])
}
return(smooth)
}
library(matlib)
contribution = matrix(0L, nrow = dim(char[,-1])[1], ncol = dim(char[,-1])[2])
for (i in 1:dim(char)[1]){
char_app = which(char[i,-1] == 1)
contribution[i, char_app] = sapply(char_app, kernel)
}
# ridge regression
lambdas = 10^seq(2,-3, by= -0.1)
cost = 0
y = t(data.matrix(senti_invest))
y = t(data.matrix(senti_invest))[rang_invest[1]:range_invest[2]]
X = contribution[,rang_invest[1]:range_invest[2]]
for (lambda in lambdas){
y_hat = t(X) %*% inv(X) %*% t(X) + lambda * diag(dim(X)[1]) %*% X %*% y
cost = c(cost ,norm(y-y_hat, type= "2"))
}
for (lambda in lambdas){
y_hat = t(X) %*% inv(X %*% t(X) + lambda * diag(dim(X)[1])) %*% X %*% y
cost = c(cost ,norm(y-y_hat, type= "2"))
}
cost = cost[-1]
lambda_choose = lambdas[which.min(cost)]
beta = inv(X %*% t(X) + lambda_choose * diag(dim(X)[1])) %*% X %*% y
beta = beta/norm(beta, type='2')
plot(beta)
range_invest = ends[c((1),(61+1)),2] # FROM where to where
lambdas = 10^seq(2,-3, by= -0.1)
cost = 0
y = t(data.matrix(senti_invest))[rang_invest[1]:range_invest[2]]
X = contribution[,rang_invest[1]:range_invest[2]]
for (lambda in lambdas){
y_hat = t(X) %*% inv(X %*% t(X) + lambda * diag(dim(X)[1])) %*% X %*% y
cost = c(cost ,norm(y-y_hat, type= "2"))
}
cost = cost[-1]
lambda_choose = lambdas[which.min(cost)]
beta = inv(X %*% t(X) + lambda_choose * diag(dim(X)[1])) %*% X %*% y
beta = beta/norm(beta, type='2')
plot(beta)
char[,1]
plot(beta)
text(beta, char[,1], cex=0.6, pos=4, col="red")
plot(beta,1:14)
plot(1:14, beta)
plot(1:14, beta)
text(b:14, beta, char[,1], cex=0.6, pos=4, col="red")
plot(1:14, beta)
text(1:14, beta, char[,1], cex=0.6, pos=4, col="red")
plot(1:14, beta, xlim = c(0,15))
plot(1:14, beta, xlim = c(0,15))
text(1:14, beta, char[,1], cex=0.6, pos=4, col="red")
plot(1:14, beta, xlim = c(0,15))
text(1:14, beta, char[,1], cex=0.6, pos=4, col="blue")
plot(1:14, beta, xlim = c(0,15))
text(1:14, beta, char[,1], cex=0.6, pos=3, col="blue")
plot(1:14, beta, xlim = c(0,15))
text(1:14, beta, char[,1], cex=0.6, pos=2, col="blue")
plot(1:14, beta, xlim = c(0,14))
text(1:14, beta, char[,1], cex=0.6, pos=2, col="blue")
plot(1:14, beta, xlim = c(0,14), main = 'overall negative sentiment contribution')
text(1:14, beta, char[,1], cex=0.6, pos=2, col="blue")
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
?cv.glmnet
setwd("~/GitHub/CVEngineer_Bootcamp")
?glmnet
senti <- read.csv("~/GitHub/CVEngineer_Bootcamp/sentiment_sentence.csv")
char <- read.csv("~/GitHub/CVEngineer_Bootcamp/character_sentence.csv")
ends <- read.csv("~/GitHub/CVEngineer_Bootcamp/chapter_ends.csv")
library(dplyr)
senti_invest = senti[2,-1] # anger
#senti_compound[,darcy_app]
x = seq(-10, 10, by = 1)
y = 10*dnorm(x, mean = 0, sd = 1)
kernel <- function(id, one_d_data = senti_invest, weight = y, window = 21){
smooth = 0
if (id >= (window-1)/2 & id <= (dim(one_d_data)[2] - (window-1)/2)){
smooth = sum(weight * one_d_data[1,(id-(window-1)/2):(id+(window-1)/2)])
} else if (id < (window-1)/2) {
smooth = sum(weight[((window+1)/2-id):window] * one_d_data[1,1:(id+(window-1)/2)])
} else {
length  = (window+1)/2 + dim(one_d_data)[2]-id
smooth = sum(weight[1:length] * one_d_data[1,id-((window-1)/2):id-((window-1)/2)+length-1])
}
return(smooth)
}
library(matlib)
contribution = matrix(0L, nrow = dim(char[,-1])[1], ncol = dim(char[,-1])[2])
for (i in 1:dim(char)[1]){
char_app = which(char[i,-1] == 1)
contribution[i, char_app] = sapply(char_app, kernel)
}
range_invest = ends[c((1),(10+1)),2] # FROM where to where
lambdas <- 10^seq(2, -3, by = -.1)
y = t(data.matrix(senti_invest))[range_invest[1]:range_invest[2]]
X = contribution[,range_invest[1]:range_invest[2]]
cv_ridge <- cv.glmnet(X, y, alpha = 0, lambda = lambdas)
X
X = t(contribution[,range_invest[1]:range_invest[2]])
X[1,]
X[2,]
X[3,]
X[4,]
X[5,]
X[6,]
X[14,]
cv_ridge <- cv.glmnet(X, y, alpha = 0, lambda = lambdas)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda
ridge_reg = glmnet(X,y, alpha = 0, family = 'gaussian', lambda = optimal_lambda)
ridge_reg$beta
View(ridge_reg)
ridge_reg[["beta"]]
beta = ridge_reg[["beta"]]
beta = as.vector(ridge_reg[["beta"]])
beta = beta/norm(beta, type='2')
plot(1:14, beta, xlim = c(0,14), main = 'overall negative sentiment contribution')
text(1:14, beta, char[,1], cex=0.6, pos=2, col="blue")
alpha_choice = c(0,1)
alpha_choice = c(0,1)
y = t(data.matrix(senti_invest))[range_invest[1]:range_invest[2]]
X = t(contribution[,range_invest[1]:range_invest[2]])
cv_ridge <- cv.glmnet(X, y, alpha = alpha_choice[2], lambda = lambdas)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda
ridge_reg = glmnet(X,y, alpha = alpha_choice[2], family = 'gaussian', lambda = optimal_lambda)
beta = as.vector(ridge_reg[["beta"]])
beta = beta/norm(beta, type='2')
plot(1:14, beta, xlim = c(0,14), main = 'overall negative sentiment contribution')
text(1:14, beta, char[,1], cex=0.6, pos=2, col="blue")
View(char)
beta
library(reticulate)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
senti <- read.csv("~/GitHub/CVEngineer_Bootcamp/sentiment_sentence.csv")
char <- read.csv("~/GitHub/CVEngineer_Bootcamp/character_sentence.csv")
ends <- read.csv("~/GitHub/CVEngineer_Bootcamp/chapter_ends.csv")
senti_invest = senti[2,-1] # anger
#senti_compound[,darcy_app]
x = seq(-10, 10, by = 1)
y = 10*dnorm(x, mean = 0, sd = 1)
kernel <- function(id, one_d_data = senti_invest, weight = y, window = 21){
smooth = 0
if (id >= (window-1)/2 & id <= (dim(one_d_data)[2] - (window-1)/2)){
smooth = sum(weight * one_d_data[1,(id-(window-1)/2):(id+(window-1)/2)])
} else if (id < (window-1)/2) {
smooth = sum(weight[((window+1)/2-id):window] * one_d_data[1,1:(id+(window-1)/2)])
} else {
length  = (window+1)/2 + dim(one_d_data)[2]-id
smooth = sum(weight[1:length] * one_d_data[1,id-((window-1)/2):id-((window-1)/2)+length-1])
}
return(smooth)
}
library(matlib)
contribution = matrix(0L, nrow = dim(char[,-1])[1], ncol = dim(char[,-1])[2])
for (i in 1:dim(char)[1]){
char_app = which(char[i,-1] == 1)
contribution[i, char_app] = sapply(char_app, kernel)
}
# contribution 14*5584
library(glmnet)
# ridge regression
range_invest = ends[c((1),(10+1)),2] # FROM where to where
lambdas <- 10^seq(2, -3, by = -.1)
alpha_choice = c(0,1)
y = t(data.matrix(senti_invest))[range_invest[1]:range_invest[2]]
X = t(contribution[,range_invest[1]:range_invest[2]])
cv_ridge <- cv.glmnet(X, y, alpha = alpha_choice[2], lambda = lambdas)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda
ridge_reg = glmnet(X,y, alpha = alpha_choice[2], family = 'gaussian', lambda = optimal_lambda)
beta = as.vector(ridge_reg[["beta"]])
beta = beta/norm(beta, type='2')
plot(1:14, beta, xlim = c(0,14), main = 'overall negative sentiment contribution')
text(1:14, beta, char[,1], cex=0.6, pos=2, col="blue")
beta
y = t(data.matrix(senti_invest))[range_invest[1]:range_invest[2]]
X = t(contribution[,range_invest[1]:range_invest[2]])
cv_ridge <- cv.glmnet(X, y, alpha = alpha_choice[1], lambda = lambdas)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda
ridge_reg = glmnet(X,y, alpha = alpha_choice[1], family = 'gaussian', lambda = optimal_lambda)
beta = as.vector(ridge_reg[["beta"]])
beta = beta/norm(beta, type='2')
plot(1:14, beta, xlim = c(0,14), main = 'overall negative sentiment contribution')
text(1:14, beta, char[,1], cex=0.6, pos=2, col="blue")
beta
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
library(astsa)
library(Rsolnp)
library(forecast)
library(tseries)
library(lubridate)
library(xtable)
library(fGarch)
library(rugarch)
library(tensorflow)
library(keras)
#### Read the data, which contain weekly log return of SP500 index and 10 stocks
load("StockData.Rdata")
# Dataset "StockData.test" will be used for grading.
# Don't delete the next two comments
#load("StockData.test.Rdata")
#StockData <-StockData.test
n1 <- 208 #the length of train set
n2 <- 52  # the length of test set
p <- dim(StockData)[2]  # The first column is the log return for SP500
# Testing periods for recording your forecasts
index.test.Step1 <- (n1+1):(n1+n2)
index.test.Step2 <- (n1+2):(n1+n2)
index.test.Step3 <- (n1+3):(n1+n2)
# Variables for mean squared errors
MSE <- rep(0,3) # The mean squared errors for 1-Step, 2-Step, 3-Step forecasts
MSE.Stock <- matrix(rep(0,(p-1)*3),3,(p-1)) # MSE for each stock
B1_MSE.Stock <- matrix(rep(0,(p-1)*3),3,(p-1))
for(i in 1:10){
for(j in 1:3){
temp = StockData[(n1+j):(n1+n2),1+i]
temp2 = sum((temp)^2)
B1_MSE.Stock[j,i] = temp2
}
}
MSE_h = apply(B1_MSE.Stock,mean, MARGIN = 1)
VAR_h = apply(B1_MSE.Stock,var, MARGIN = 1)
B1 = data.frame(MSE = MSE_h, VAR = VAR_h)
View(B1)
View(B1_MSE.Stock)
View(B1_MSE.Stock)
View(B1)
idx = StockData[,1]
idx_invest = ts(StockData[,2],
freq=365.25/7,
start=decimal_date(ymd("2000-01-01")))
lag_transform <- function(x, k= 1){
lagged =  c(rep(NA, k), x[1:(length(x)-k)])
DF = as.data.frame(cbind(lagged, x))
colnames(DF) <- c( paste0('x-', k), 'x')
DF[is.na(DF)] <- 0
return(DF)
}
supervised = lag_transform(idx_invest, 1)
head(supervised)
lag_transform <- function(x, k= 1){
lagged =  c(rep(NA, k), x[1:(length(x)-k)])
DF = as.data.frame(cbind(lagged, x))
colnames(DF) <- c( paste0('x-', k), 'x')
DF[is.na(DF)] <- 0
return(DF)
}
supervised = lag_transform(idx_invest, 1)
#head(supervised)
# split to training set and testing set
train = supervised[1:n1, ]
test  = supervised[(n1+1):(n1+n2),  ]
scale_data = function(train, test, feature_range = c(0, 1)) {
x = train
fr_min = feature_range[1]
fr_max = feature_range[2]
std_train = ((x - min(x) ) / (max(x) - min(x)  ))
std_test  = ((test - min(x) ) / (max(x) - min(x)  ))
scaled_train = std_train *(fr_max -fr_min) + fr_min
scaled_test = std_test *(fr_max -fr_min) + fr_min
return( list(scaled_train = as.vector(scaled_train), scaled_test = as.vector(scaled_test) ,scaler= c(min =min(x), max = max(x))) )
}
Scaled = scale_data(train, test, c(-1, 1))
y_train = Scaled$scaled_train[, 2]
x_train = Scaled$scaled_train[, 1]
y_test = Scaled$scaled_test[, 2]
x_test = Scaled$scaled_test[, 1]
invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){
min = scaler[1]
max = scaler[2]
t = length(scaled)
mins = feature_range[1]
maxs = feature_range[2]
inverted_dfs = numeric(t)
for( i in 1:t){
X = (scaled[i]- mins)/(maxs - mins)
rawValues = X *(max - min) + min
inverted_dfs[i] <- rawValues
}
return(inverted_dfs)
}
# Reshape the input to 3-dim
dim(x_train) <- c(length(x_train), 1, 1)
# specify required arguments
X_shape2 = dim(x_train)[2]
X_shape3 = dim(x_train)[3]
batch_size = 1                # must be a common factor of both the train and test samples
units = 1                     # can adjust this, in model tuninig phase
#=========================================================================================
model <- keras_model_sequential()
model%>%
layer_lstm(units, batch_input_shape = c(batch_size, X_shape2, X_shape3), stateful= TRUE)%>%
layer_dense(units = 1)
model %>% compile(
loss = 'mean_squared_error',
optimizer = optimizer_adam( lr= 0.02, decay = 1e-6 ),
metrics = c('accuracy')
)
summary(model)
Epochs = 50
for(i in 1:Epochs ){
model %>% fit(x_train, y_train, epochs=1, batch_size=batch_size, verbose=1, shuffle=FALSE)
model %>% reset_states()
}
L = length(x_test)
scaler = Scaled$scaler
predictions = numeric(L)
for(i in 1:L){
X = x_test[i]
dim(X) = c(1,1,1)
yhat = model %>% predict(X, batch_size=batch_size)
# invert scaling
yhat = invert_scaling(yhat, scaler,  c(-1, 1))
# store
predictions[i] <- yhat
}
result = data.frame(real = idx_invest, predicted = c(rep(NA, n1), predictions))
matplot(result, type = c("b"),pch=1,col = 1:4) #plot
check = result[(n1+1):(n1+n2),]
sum((check[,1] - check[,2])^2)
