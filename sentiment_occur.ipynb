{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "#!conda install nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import scipy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#import ssl\n",
    "#ssl._create_default_https_context = ssl._create_unverified_context\n",
    "#nltk.download('punkt')\n",
    "\n",
    "#!pip3 install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pride_and_prejudice.txt','r') as f:\n",
    "#with open('Pride and Prejudice.txt','r') as f:\n",
    "    full_txt = f.read().strip('\\n').replace(\"\\''\",\"'\")\n",
    "    # clean the unnecessary txt\n",
    "    full_txt = full_txt.replace('_','').replace('make by 拉米网（www.lami.fun）','') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chapter-wise\n",
    "chapter_txt = full_txt.split('Chapter ')[1:] #len(chapter_txt)=61\n",
    "cleaned_chapter = []\n",
    "for chapter in chapter_txt:\n",
    "    cleaned_chapter.append(chapter[1:].replace('\\n',''))\n",
    "len(cleaned_chapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5584"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence-wise\n",
    "cleaned_sentence = []\n",
    "from nltk.tokenize import sent_tokenize\n",
    "cleaned_sentence = [sent_tokenize(chapter) for chapter in cleaned_chapter]\n",
    "cleaned_sentence = [x for y in cleaned_sentence for x in y]\n",
    "len(cleaned_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_character_sentence(name, nick_name = None):\n",
    "    if nick_name != None:\n",
    "        sentence = [x for x in cleaned_sentence if (x.find(name)!=-1) or (x.find(nick_name)!=-1)]\n",
    "        return sentence\n",
    "    else:\n",
    "        sentence = [x for x in cleaned_sentence if x.find(name)!=-1]\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(find_character_sentence('Darcy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARACTER_LIST = [\n",
    "    'Mr. Darcy', 'Mr. Bingley','Elizabeth','Miss Bingley','Lydia','Catherine', # 0 ~ 5\n",
    "    'Mary','Georgiana','Collins','Charlotte','Mr. Wickham',         #6 ~ 10\n",
    "    'Miss de Bourgh','Mr. Bennet','Mrs. Bennet','Jane','Louisa',    #11 ~ 15\n",
    "    'Mr. Hurst','Lucases','Miss Lucas','Sir William','Mrs. Gardiner',  # 16 ~20\n",
    "    'Mr. Gardiner','Mr. Phillips','Mrs. Phillips','Fitzwilliam','Colonel Forster', #21~25\n",
    "    'Mrs. Forster','Mr. Jones','Lady Catherine'   #26～28\n",
    "]\n",
    "\n",
    "NICKNAME_LIST = [\n",
    "    'Lizzy','Kitty','Miss Lucas','Caroline','Miss Darcy','Lady Lucas','Mrs. Hurst','Charles','Darcy','Bingley'\n",
    "]\n",
    "\n",
    "MATCH = [(2,0),(5,1),(9,2),(3,3),(6,4),(8,5),(14,6),(1,7),(0,8),(1,9)]\n",
    "                                        \n",
    "num_people = len(CHARACTER_LIST)                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_plus_nick(list_1,list_2):\n",
    "    co_list = []\n",
    "    len_1 = len(list_1)\n",
    "    for i in range(len_1):\n",
    "        if list_1[i]+list_2[i] == 0:\n",
    "            co_list.append(0)\n",
    "        else:\n",
    "            co_list.append(1)\n",
    "    return co_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_interaction(sections):\n",
    "    Characters = {}\n",
    "    occurrence = []\n",
    "    occurrence_2 = []\n",
    "    for person in CHARACTER_LIST:\n",
    "        vector = []\n",
    "        for section in sections:\n",
    "            if person in section:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        occurrence.append(np.array(vector))\n",
    "        Characters[person] = np.array(vector)\n",
    "    \n",
    "    for person in NICKNAME_LIST:\n",
    "        vector = []\n",
    "        for section in sections:\n",
    "            if person in section:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        occurrence_2.append(np.array(vector))\n",
    "    \n",
    "    for match in MATCH:\n",
    "        name = match[0]\n",
    "        nick = match[1]\n",
    "        occur_new = name_plus_nick(occurrence[name],occurrence_2[nick])\n",
    "        occurrence[name] = occur_new\n",
    "        person = CHARACTER_LIST[name]\n",
    "        Characters[person] = occur_new\n",
    "\n",
    "    occurrence_matrix = pd.DataFrame(occurrence) # num of people * num of sections 15*61\n",
    "    co_occur = pd.DataFrame(np.dot(occurrence_matrix,occurrence_matrix.T))\n",
    "    co_occur = co_occur / np.linalg.norm(co_occur, ord = 1, axis = 0)\n",
    "    return co_occur, occurrence_matrix, Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_rank(section_name, section):\n",
    "    co_occur, occurrence, Characters = find_interaction(section)\n",
    "    #print(co_occur)\n",
    "    P = co_occur\n",
    "    alpha = 0.85\n",
    "    P = alpha* co_occur + (1-alpha)*np.ones((num_people,num_people))/num_people\n",
    "    \n",
    "    # interation PageRank\n",
    "    rank_inter = np.ones((num_people, 1))/num_people\n",
    "    for i in range(100000):\n",
    "        rank_inter = np.dot(P,rank_inter)\n",
    "    rank_inter = rank_inter.reshape(-1,)\n",
    "    \n",
    "    text_rank = rank_inter[np.argsort(rank_inter)[::-1]].tolist()\n",
    "    text_chara = [CHARACTER_LIST[i] for i in np.argsort(rank_inter)[::-1].tolist()]\n",
    "    Characters_sort = sorted(Characters.items(), key=lambda x: sum(x[1]), reverse = True)\n",
    "    occur_rank = [sum(character[1]) for character in Characters_sort]\n",
    "    occur_chara = [character[0] for character in Characters_sort]\n",
    "  \n",
    "    return occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "occur_sentence = text_rank('sentences', cleaned_sentence)\n",
    "occur_sentence.index = CHARACTER_LIST\n",
    "occur_sentence.to_csv('./character_sentence.csv') # character * sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "occur_chapter = text_rank('chapters', cleaned_chapter)\n",
    "occur_chapter.index = CHARACTER_LIST\n",
    "occur_chapter.to_csv('./character_chapter.csv') # character * chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-267ce7a1f5d3>:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chapter in tqdm(cleaned_chapter):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22796e94fae417cac5df9492b3b63c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "chapter_ends = [0]\n",
    "sentiments = {'compound': [], 'neg': [], 'neu': [], 'pos': []}\n",
    "for chapter in tqdm(cleaned_chapter):\n",
    "    sentence_list = nltk.tokenize.sent_tokenize(chapter)\n",
    "    chapter_ends.append(chapter_ends[-1]+len(sentence_list))\n",
    "    \n",
    "    for sentence in sentence_list:\n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        sentiments['compound'].append(vs['compound'])\n",
    "        sentiments['neg'].append(vs['neg'])\n",
    "        sentiments['neu'].append(vs['neu'])\n",
    "        sentiments['pos'].append(vs['pos'])\n",
    "    \n",
    "pd.DataFrame(chapter_ends).to_csv('./chapter_ends.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_matrix = pd.DataFrame(sentiments).T \n",
    "sentiment_matrix.to_csv('./sentiment_sentence.csv') # sentiment * sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
